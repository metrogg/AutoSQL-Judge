# AutoSQL-Judge 项目深度复盘与面试通关指南

> 面向角色：准备用本项目冲击中高级开发岗位的你。目标是：**真正搞懂这个系统在干什么、为什么这么干，以及面试官会怎么问你。**

---

## 第一部分：全局架构与技术选型深度解析（The "Why"）

### 1. 架构总览：从浏览器到数据库再到大模型

一句话概括整个系统：

> 这是一个典型的 **B/S 架构在线评测系统**，用 Flask 提供 HTTP API，前端一个 `index.html` + Vue 3 管理所有交互，MySQL + Pandas 负责 SQL 执行与结果比对，大模型只负责“出题”和“讲解”，不参与打分。

可以从三条主链路来理解：

#### 1.1 学生做题提交流程

1. **前端输入与状态管理**  
   - 右侧是 `Monaco Editor` 深色主题 SQL 编辑器，由 Vue 3 通过 `ref(sqlCode)` 等响应式变量管理内容。  
   - 学生点击“运行”按钮时，前端从 Vue 状态里取出：
     - 当前题目 ID / 难度 / 分值；
     - 当前数据集标识（如 `student_scores`）；
     - 当前编辑器里的 SQL 文本 `sqlCode`。  
   - 然后使用 Axios 发送 POST 请求到 Flask，例如 `/api/judge/submit`。

2. **HTTP 请求到达后端**  
   - 浏览器发起 HTTP/1.1 请求，带上 Cookie（里边是 Flask 的 session ID）。  
   - WSGI 服务器（比如 gunicorn/uWSGI，开发环境下是 Flask 内置 server）接收到请求，按照 WSGI 协议调用 Flask 应用。  
   - Flask 根据路由把请求分发给视图函数（如 `api_judge_submit`），视图函数解析 JSON body，做参数校验。

3. **判题核心：SQL 执行 + Pandas 结果集比对**  
   - 从系统库 `sql_exam_sys` 的 `questions` 表中，根据 `question_id` 取出：  
     - 标准答案 SQL（`standard_sql`）；
     - 关联的数据集键（`dataset_key`）。  
   - 通过 `utils/db.py` 封装好的连接管理，从 `datasets` 配置里找到对应靶场库（如 `ds_student_scores`），拿到 SQLAlchemy Engine。  
   - 使用 Pandas 的 `read_sql`：  
     - `df_std = pd.read_sql(standard_sql, engine)`  
     - `df_user = pd.read_sql(user_sql, engine)`  
   - 对两个 DataFrame 执行：
     - 统一排序：`sort_values(by=所有列).reset_index(drop=True)`；
     - 再调用 `df_std.equals(df_user)` 做严格相等比对。  
   - 构造判题结果结构（是否通过、得分、执行时间、错误信息），同时插入一条记录到 `records` 表。

4. **后端返回 JSON 响应**  
   - 典型的返回数据结构类似：
     ```json
     {
       "status": "success",
       "data": {
         "result": "pass",
         "score": 10,
         "exec_time_ms": 123,
         "message": "恭喜，答案正确！"
       }
     }
     ```

5. **前端渲染判题结果**  
   - Axios 拿到响应后，更新 Vue 里的 `judgeResult`、`submissionHistory` 等状态：
     - 自动切换右侧 Tab 到“执行结果”；
     - 顶部显示绿色/橙色/红色状态条；
     - 底部展示结果预览表格、得分变动和“AI 讲解 / 查看标准答案”等按钮。

这条链路体现了一个关键设计：**“裁判”是可验证的程序，而不是大模型**。

---

#### 1.2 AI 出题链路

1. 学生在左侧“AI 出题对话”中，以自然语言描述希望练习的场景。  
2. 前端将这段描述、目标数据集 key 等信息打包，通过 Axios 调用后端 `/api/ai/generate_question`。  
3. 后端：
   - 根据 `dataset_key`，从配置/数据库中取得该数据集的 Schema 描述（表名、字段名、字段类型）。
   - 将 Schema 展平为易读文本：
     ```text
     表名: students
     - student_id (INT)
     - name (VARCHAR)
     ...
     表名: scores
     - student_id (INT)
     - course_id (INT)
     - score (DECIMAL)
     ```
   - 设计 Prompt，调用 DeepSeek / OpenAI 兼容 API，请求模型返回**严格 JSON** 格式的题目信息：
     - `title` 题目标题；
     - `difficulty` 难度；
     - `standard_sql` 标准答案；
     - `description` 题目描述等。  
   - 收到模型输出后，先在靶场库中执行一遍 `standard_sql`，确保可执行且结果合理，失败时视为该题目无效，可重试或提示用户。
   - 校验通过后，把题目写入 `questions` 表，返回给前端。

4. 前端拿到题目对象后，更新：
   - 左侧“当前题目”卡片（标题、难度徽章、分值等）；
   - 当前数据集展示与 Schema 列表；
   - 清空/重置右侧编辑器中的历史 SQL。

这条链路的核心思想是：**永远用“数据库是否能执行”作为大模型输出的硬验收标准**。

---

#### 1.3 AI 讲解链路

1. 学生提交 SQL 得到失败/错误结果后，点击“AI 讲解”。  
2. 前端收集上下文：
   - 题目 ID/标题；
   - 用户 SQL；
   - 判题结果（pass/fail/error）；
   - 判题消息（错误日志）。
   并发送到 `/api/judge/explain`。

3. 后端：
   - 从 `Question` 表中提取标题和 `standard_sql`；
   - 通过 `db_manager.get_dataset_schema(dataset_key)` 拼出 Schema 文本；
   - 调用 `explain_sql_answer(...)`：
     - 传入 `schema_text / title / standard_sql / user_sql / result / judge_message`；
     - 内部封装 LLM API 调用，使用精心设计的 Prompt，让模型输出结构化讲解：
       - 错误点分析；
       - 正确思路；
       - 相关知识点提示；
       - 可选优化建议。

4. 后端返回：
   ```json
   {
     "status": "success",
     "data": { "feedback": "...详细讲解文本..." }
   }
   ```

5. 前端在右侧结果区或悬浮 AI 助教面板中渲染这段讲解，支持多轮追问。

这里的关键点是：**讲解的依据是“判题结果 + 两份 SQL + Schema”，模型是“辅导老师”，而不是“裁判”。**

---

### 2. 技术选型辩护

#### 2.1 前端：Vue 3 CDN + 原生 HTML/CSS + Axios + Monaco

**为什么不用 React + 打包工具？**

- 项目定位是**课设/个人项目**，希望部署简单、依赖少：
  - 不引入 webpack/Vite，可直接在 Flask 模板中写前端逻辑；
  - 不需要单独的 Node 构建环境，方便在任何实验机房或同学电脑快速运行。  
- Vue 3 CDN 版本 + 组合式 API：
  - 在一个 `createApp()` 中集中管理所有状态（题目、Schema、SQL、判题结果、AI 聊天记录等）；
  - 响应式系统负责 DOM 更新，不需要手动操作 DOM；
  - 占用体积相对可控，学习曲线对已有 JS 基础的同学较友好。

**为什么选 Monaco Editor 而不是 `<textarea>`？**

- Monaco 是 VS Code 的编辑器内核：
  - 支持 SQL 语法高亮、行号、缩进、基本补全和键盘快捷键；
  - UI 体验和主流在线刷题平台保持一致。  
- 从工程角度：
  - 使用 `loader.js` + AMD 方式加载，需要在 Vue 的 `onMounted` 中初始化，确保 DOM 已经渲染；
  - 配置 `require.config` 的 `vs` 路径时切换到 `unpkg` CDN，解决了原先加载失败的问题；
  - 设计了降级策略：`window.require` 不存在时自动降级为 `<textarea>`，并通过 toast 提示用户。

**为什么布局要仿牛客网？**

- 左题右码 + 底部 `执行结果 / 自测输入 / 提交记录` Tab + 运行按钮，是在线判题领域已经被验证过的交互范式：
  - 学生几乎不需要学习成本就能上手；
  - UI 一致性增强了项目的“专业感”和“产品感”。

总体上，你可以总结为：

> 为了兼顾**开发效率、部署简单和用户体验**，我选择了 Vue 3 CDN + Monaco 的轻量前端方案，而不是上来就用复杂的前端工程化工具链。

---

#### 2.2 后端：Python + Flask + SQLAlchemy + Pandas

**为什么后端选 Python + Flask？**

- Python 拥有成熟的数据处理生态（Pandas、NumPy）和 AI 生态（各类 LLM SDK），非常适合“SQL + 大模型”的组合场景。  
- Flask 足够轻量：
  - 路由 = 函数，学习成本低；
  - 插件生态丰富（登录、CORS、数据库等）。

**为什么用 SQLAlchemy + Pandas 组合？**

- SQLAlchemy：
  - 负责系统库的 ORM 映射，统一模型层（User / Question / Record / Dataset等）；
  - 管理连接池，封装 SQL 细节，支持未来迁移到其他数据库。  
- Pandas：
  - 判题本质是“结果集级的精确比较”，表现在代码上就是“两个 DataFrame 是否相同”；
  - DataFrame 是内存中的列式结构，适合对小规模结果做排序、重置索引和逐元素比较；
  - 将“判题逻辑”写在 Python 层，而不是写复杂的 SQL 存储过程，可读性更高。

**MySQL 分库设计：系统库 vs 靶场库**

- 系统库 `sql_exam_sys`：存放用户信息、题目元数据和提交记录；
- 靶场库 `ds_student_scores` 等：存放各类业务数据集，只读访问。  
- 好处：
  - 权限隔离：靶场库可以使用只读账号，即使 SQL 写错也无法破坏数据；
  - 扩展性高：方便将来增加新的场景（电商、图书馆等）而不影响系统库结构。

一句话总结：

> **Flask + SQLAlchemy** 保证应用结构清晰，**Pandas** 让判题逻辑在 Python 世界里高度可读和可控，**MySQL 分库**提供了安全与扩展性的基础设施。

---

#### 2.3 大模型集成：让 LLM 做“助教”而不是“裁判”

你刻意做了一个非常重要的架构决策：

> 判分不依赖 LLM，LLM 只负责出题和讲解。

**为什么？**

- 判题结果必须**可验证**：
  - SQL 判题的核心是“结果是否一致”，用数据库 + Pandas 可以做到精确比对；
  - 如果用 LLM 来判断“这条 SQL 对不对”，就会引入不可控的幻觉和主观性。

- LLM 更适合做“语言生成”和“知识表达”：
  - 出题：根据 Schema 设计题干、难度和标准 SQL；
  - 讲解：根据 Schema + 用户 SQL + 标准 SQL + 判题结果，生成结构化、易懂的讲解。

**API 层面的封装**

- 统一封装一个 `llm_client`：
  - 支持配置切换 DeepSeek / OpenAI 等兼容接口（`base_url` + `api_key`）；
  - 提供清晰的函数：`generate_question(...)`、`explain_sql_answer(...)`；
  - 内部处理超时、重试、异常捕获，把错误转换成统一的 JSON 返回给前端。

**Prompt 设计要点**

- 出题 Prompt：
  - 明确指定输出格式为 JSON；
  - 提供 Schema + 难度要求 + 题目风格；
  - 要求生成 MySQL 下可执行的标准 SQL。  
- 讲解 Prompt：
  - 输入包括 Schema 文本、题目标题、标准 SQL、用户 SQL、判题结果和错误信息；
  - 要求输出分为：错误原因、正确思路、知识点总结和改进建议。

**未来扩展到 RAG**（面试时可以主动提）：

- 把 SQL 教程、经典错题解析等内容放入向量库；
- 用户提问时先检索相关段落，再与 Schema + SQL 一起喂给 LLM；
- 这样可以进一步降低幻觉、提高讲解的一致性和可控性。

---

## 第二部分：核心代码逻辑与难点攻克（The "How"）

### 难点一：在 Flask 模板中集成 Vue 3 + Monaco + 牛客风布局

#### 1. 模板引擎冲突：Jinja2 vs Vue

- Flask 的 Jinja2 模板使用 `{{ ... }}`，Vue 也使用 `{{ ... }}`：
  - 如果直接写，会导致 Jinja 抢先把表达式解析掉。  
- 解决方案：
  - 使用 `{% raw %} ... {% endraw %}` 把 Vue 模板包起来：
    ```jinja2
    {% raw %}
    <div id="app">
      {{ some_vue_state }}
    </div>
    {% endraw %}
    ```
  - 这样 Jinja 会原样输出，真正的解析由浏览器里运行的 Vue 完成。

这个细节非常适合作为面试中的“坑点经验”。

---

#### 2. Monaco Editor 的加载与降级策略

- 使用 `loader.js` 和 AMD 方式加载 Monaco：
  - 需要在全局配置：
    ```js
    require.config({ paths: { vs: 'https://unpkg.com/monaco-editor@x.y.z/min/vs' } });
    ```
- 在 Vue 组件的 `onMounted` 钩子中初始化编辑器：
  - 确保 DOM 已经渲染，容器拥有正确的宽高；
  - 典型初始化过程：
    ```js
    onMounted(() => {
      if (!window.require) {
        useMonaco.value = false; // 降级
        showToast('代码编辑器加载失败，已使用简易文本框');
        return;
      }
      window.require(['vs/editor/editor.main'], () => {
        editor.value = monaco.editor.create(...);
      });
    });
    ```

- 降级策略：
  - 当 `window.require` 不存在时：
    - 不再尝试创建 Monaco；
    - 显示 `<textarea class="sql-editor-textarea">` 作为回退；
    - 用 Toast 明确提示用户：**功能可用，但体验略有降级**。

这个设计体现了你对“**可用性优先于体验**”的工程思维。

---

#### 3. 牛客风右侧布局与拖拽分栏

- 右侧整体结构：
  - 顶部：`editor-topbar`，显示“SQL 编辑器”和当前数据集徽章；
  - 中部：`editor-wrapper`，放 Monaco Editor 容器；
  - 底部：`editor-tabs` + `tab-panel`，承载 Tab 按钮和执行结果内容。  
- 布局技巧：
  - 将 `editor-container` 设为 `display: flex; flex-direction: column;`；
  - 对 `editor-wrapper` 和 `tab-panel` 使用 `flex: 1; min-height: 0;` 防止内容撑破布局；
  - Tab 工具条使用一行布局，左侧是 Tab 按钮，右侧是“运行”按钮，使之在视觉上形成单一“底部工具条”。

- 中间分隔条拖拽：
  - 分隔条元素绑定 `mousedown` 事件：设置 `isResizing = true`；
  - 在 `window` 上监听 `mousemove`：
    - 当 `isResizing` 为 true 时，根据 `clientX` 计算左侧占比；
    - 通过 Vue 状态（如 `leftPanelWidthPercent`）影响 `.left-panel` 的 flex-basis；
  - 在 `window` 上监听 `mouseup`：将 `isResizing` 置回 false。

**底层原理补充（面试中可讲）：**

- 在 flex 布局中，`min-height: 0` 能防止子元素内容过长时“挤爆”父容器，使滚动行为在预期区域内发生。  
- 拖拽事件可能频繁触发重排（reflow），如果将来性能成为瓶颈，可以在 `mousemove` 处理函数上增加节流（throttle）。

---

### 难点二：安全且高可读的 SQL 判题内核

#### 1. 安全性设计

- **只读账号**：
  - 靶场库使用权限极度收缩的只读账号，任何 `INSERT/UPDATE/DELETE/DDL` 操作直接在数据库层被拒绝。  
- **SQL 关键字过滤**：
  - 在后端对 `user_sql` 做简单的关键字检测，拒绝包含高危操作的语句；
  - 虽然不是绝对安全，但配合只读账号足以满足教学场景的安全要求。  
- **结果集大小/执行时间限制（可扩展点）**：
  - 可以为查询设定最大返回行数或执行时间（目前可以作为未来优化点，在面试时主动提）。

#### 2. 判题逻辑：DataFrame 级别的比较

- 两个 SQL：标准答案 SQL 和用户 SQL；
- 执行后得到 `df_std` 和 `df_user`：
  - 都是 DataFrame，列名和类型由实际 SQL 决定。
- 判题步骤：
  1. 对两个 DataFrame 按所有列排序：
     ```python
     df_std = df_std.sort_values(by=df_std.columns.tolist()).reset_index(drop=True)
     df_user = df_user.sort_values(by=df_user.columns.tolist()).reset_index(drop=True)
     ```
  2. 使用 `df_std.equals(df_user)` 做严格比较：
     - 比较行数、列数、列名、数据值，甚至 NaN 的位置。
  3. 根据比较结果设置 `result = pass/fail`，并记录必要的统计信息（行数、执行耗时等）。

- 这种方式的优点：
  - 把“语义判断”降解成“数据结构是否相等”；
  - 代码短小、容易解释，非常适合作为**面试中的白板题**来展示；
  - 可以很自然地扩展为“找差异行”的高级特性。

#### 3. 并发与资源管理

- 使用 SQLAlchemy Engine 的连接池处理多请求并发：
  - 避免每次请求都重新建立数据库连接；
  - 可配置连接池大小和超时时间。  
- GIL 的影响：
  - 判题逻辑属于 I/O + 中等 CPU 负载；
  - 实际部署中通常使用多进程 WSGI worker，将并发拆分到多个进程，削弱 GIL 影响；
  - 若未来判题逻辑变得特别重，可以考虑异步任务队列（Celery）进一步隔离耗时操作。

一句话总结：

> 判题内核是一个“**SQL → DataFrame → 结果比较**”的流水线，重点是让逻辑**清晰、可验证、可解释**，同时在教学场景下保持足够的性能和安全性。

---

### 难点三：大模型出题与讲解的 Prompt 工程

#### 1. 出题模块：从 Schema 到标准 SQL

- 输入：
  - 目标数据集的 Schema 文本；
  - 学生的自然语言需求（场景描述）；
  - 难度/题量等参数。  
- Prompt 设计：
  - System：你是 SQL 教学专家，请基于以下 Schema 设计 SQL 查询题；
  - User：附上 Schema、场景描述和输出格式要求；
  - 明确要求返回 JSON，字段包括：
    - `title`：简短标题；
    - `difficulty`：简单/中等/困难；
    - `standard_sql`：标准答案 SQL（MySQL 语法）；
    - `description`：题目详细说明。  
- 结果校验流程：
  - 解析 LLM 输出的 JSON，如果解析失败直接认定为无效；
  - 在靶场库运行 `standard_sql` 验证其合法性和可执行性；
  - 只把通过校验的题目写入 `questions` 表并返回前端。

#### 2. 讲解模块：多信源聚合

- 输入：
  - Schema 文本；
  - 题目标题/描述；
  - 标准答案 SQL；
  - 用户 SQL；
  - 判题结果和错误日志。  
- Prompt 结构：
  - 第一段：重述题目和 Schema，帮助模型建立场景；
  - 第二段：告知模型“标准 SQL 是 X，用户 SQL 是 Y，判题结果是 fail/error”；
  - 第三段：要求输出格式，比如：
    1. 核心错误点；
    2. 正确解题思路；
    3. 相关 SQL 知识点；
    4. 可选优化建议。  

通过这种方式，你让大模型的输出更**结构化、可预测，也更容易展示在前端 UI 中**。

#### 3. 异常处理与降级

- 封装 LLM 调用时，必须处理：
  - 网络超时；
  - 429 限流；
  - 500 服务错误；
  - 模型返回不符合 JSON 约定格式。  
- 错误处理策略：
  - 后端捕获异常，返回 `{status: "error", msg: "调用大模型失败: ..."}`；
  - 前端捕获后 toast 提示：“AI 讲解暂时不可用，请稍后再试。”

整体上，这一块体现了你对大模型的理解：**它是一个不可靠但有价值的远程服务，需要完整的工程包装和降级策略。**

---

## 第三部分：该项目在简历上的“高大上”包装

以下是基于 STAR 法则提炼出的 4 条亮点，可直接写进简历（根据需要中英文混写）：

1. **基于 LLM 的智能 SQL 在线练习平台**  
   - S/T：传统 SQL 题库题量有限、维护成本高，缺乏个性化反馈。  
   - A：独立设计并实现 AutoSQL-Judge，使用 Flask + MySQL + Pandas 构建在线 SQL 判题系统，集成大模型实现题目自动生成与错误讲解，前端仿牛客网实现左题右码布局与 VS Code 内核编辑器。  
   - R：支持多数据集场景下的在线练习，单机环境下稳定服务 30+ 并发学生，课程展示中获得导师和同学一致好评。

2. **结果集级 SQL 判题内核**  
   - S/T：需要对学生提交的 SQL 实现“语义等价”判分，而不是简单的字符串比对。  
   - A：使用 SQLAlchemy 将标准答案 SQL 与用户 SQL 统一在靶场库执行，利用 Pandas 将结果加载为 DataFrame，通过排序 + 重置索引进行严格相等性比较，并记录执行耗时与错误日志。  
   - R：显著降低误判率，支持多种等价写法（JOIN/子查询/排序方式）的自动识别，为后续扩展多业务数据集提供了可复用的判题核心。

3. **VS Code 内核在线编辑器与牛客风 UI 重构**  
   - S/T：原型界面基于 `<textarea>` 和简单布局，用户体验粗糙。  
   - A：引入 Monaco Editor 替代原始文本框，基于 Vue 3 重构学生端界面：左侧 AI 出题对话 + 当前题目 + Schema 展示，右侧为深色 SQL 编辑器与底部 `执行结果 / 自测输入 / 提交记录` Tab，支持拖拽分栏和悬浮 AI 助教。  
   - R：整体交互和视觉效果对标牛客网 SQL 练习界面，在课程答辩中被评价为“产品级前端体验”，显著提升项目亮点。

4. **工程化集成大模型：出题与讲解的 Prompt 设计与封装**  
   - S/T：需要在保证判分可控的前提下，引入大模型提升题目多样性和讲解质量。  
   - A：封装 DeepSeek/OpenAI 兼容 API 客户端，设计结构化 Prompt：出题阶段强制 JSON 输出（题目 + 难度 + 标准 SQL），讲解阶段聚合 Schema/题目/两版 SQL 及判题结果生成分段式解析，并对网络异常/限流场景实现统一降级。  
   - R：构建出“程序裁判 + AI 助教”的混合评测模式，使系统既保证评分结果的确定性，又能提供接近人类助教风格的学习反馈。

---

## 第四部分：20 道地狱级面试题与参考回答

> 说明：下面的问题分为三类：基础与原理、项目实战与系统设计、挖坑与压力测试。建议你先自己尝试作答，再对照参考答案优化自己的表达方式。

### 维度 A：基础与原理（8 题）

**A1. 为什么你没有为这个项目引入 WebSocket 或 SSE 做流式输出？**  
- 要点：当前场景下，HTTP 短连接足够，流式的复杂度 > 收益；未来如需代码实时分析/补全再引入流式方案。  

**A2. HTTP 请求从浏览器发出到 Flask 视图函数，中间经历了哪些关键环节？**  
- 要点：浏览器构造 HTTP 报文 → TCP 连接 → 服务器 WSGI 容器（gunicorn/uWSGI） → 调用 Flask 应用 → 路由匹配视图 → 视图函数处理并返回 Response。  

**A3. 解释一下 ORM（SQLAlchemy）的优缺点，以及你在项目中如何使用它。**  
- 要点：ORM 提升开发效率与可维护性；代价是性能和 SQL 细颗粒度控制；本项目中用于系统库建模和连接管理，而判题仍然使用手写 SQL。  

**A4. Pandas DataFrame 与数据库表的设计目标有何不同？为什么适合做判题逻辑？**  
- 要点：数据库专注持久化与并发访问，DataFrame 专注单机内存分析；判题是一次性分析场景，非常适合拉到内存用 DataFrame 精确比对。  

**A5. GIL 会不会严重影响你这个系统的并发能力？**  
- 要点：主要瓶颈在数据库和 I/O，通过多进程 WSGI worker 可以充分利用多核，GIL 并不是这个项目的决定性瓶颈。  

**A6. Vue 3 的响应式系统原理是什么？它和你用 `ref` 管理编辑器状态有什么关系？**  
- 要点：Vue 3 基于 Proxy 拦截属性读写实现依赖收集与更新；用 `ref/ reactive` 能让 DOM 与状态自动同步，避免手动 DOM 操作。  

**A7. Cookie + Session 和 JWT 相比，有什么差异？为什么你这里用 Session 更合适？**  
- 要点：Session 数据存服务器端，更易撤销和管理权限；JWT 无状态适合微服务 / 多端共享，这里是单体系统，用 Session 更简单安全。  

**A8. 为什么必须在 Vue 的 `onMounted` 钩子中初始化 Monaco Editor？**  
- 要点：只有当 DOM 渲染并布局完成后才能获得正确容器尺寸，提前初始化会导致编辑器宽高为 0 或布局错误。

---

### 维度 B：项目实战与系统设计（7 题）

**B1. 如果并发用户突然增加 100 倍，你的系统最先会出问题在哪里？怎么优化？**  
- 要点：瓶颈大概率在数据库和判题内核；优化方向包括：SQL/索引优化、连接池参数调优、判题任务异步化与限流等。  

**B2. 判题结果的数据结构是如何设计的？前端如何根据它驱动 UI？**  
- 要点：后端统一返回 `{status, data{result, score, exec_time, message}}`，前端将 `result` 映射为不同 UI 状态条，并在 `submissionHistory` 中保留最近记录。  

**B3. 为什么不直接在 MySQL 里写一个存储过程做结果集比较？**  
- 要点：存储过程在数据库层对比性能好，但可读性和可移植性差；Pandas 让判题逻辑在 Python 层更清晰，适合教学和快速迭代。  

**B4. 你的数据库结构是如何支持多数据集、多场景扩展的？**  
- 要点：通过 `datasets` 抽象和 `dataset_key` 字段把题目与靶场库关联，系统库与靶场库分离，便于增加新数据集和控制权限。  

**B5. 大模型的幻觉会不会影响判分？你如何缓解讲解中的幻觉问题？**  
- 要点：判分完全由 MySQL + Pandas 决定，LLM 只负责讲解；通过提供 Schema/两份 SQL/判题结果等充分上下文，并限定输出格式来降低幻觉。  

**B6. 如果要把这个系统作为教学平台正式上线，你会做哪些安全与可靠性增强？**  
- 要点：严格 SQL 过滤、只读账号、敏感配置环境变量管理、速率限制、日志记录与告警、异常兜底页面等。  

**B7. 教师想要导出班级成绩统计和错题分析，你会如何设计接口与数据结构？**  
- 要点：基于 `records` 表进行聚合统计（按学生/按题目），通过 Pandas 或 SQL 实现统计查询，并提供导出 CSV/Excel 的 API 或管理端页面。  

---

### 维度 C：挖坑题与压力测试（5 题）

**C1. Pandas 会不会是对这个项目的“过度设计”？**  
- 要点：承认存在 trade-off，但说明 Pandas 在判题逻辑的可读性和灵活性上带来了明显好处，在当前数据规模下完全可接受。  

**C2. 说一个你在这个项目中遇到的最棘手 Bug，并详细讲讲你是如何排查的。**  
- 要点：可以讲 Monaco Editor 加载失败或 Vue/Jinja 模板冲突，通过浏览器 DevTools、最小化复现、查看网络请求和错误栈一步步定位并解决。  

**C3. 如果有人说你这个项目“花里胡哨，工程价值不高”，你怎么回应？**  
- 要点：承认项目规模有限，但强调你在其中刻意实践了很多真实工程问题（安全、扩展性、大模型集成、体验优化），并提出如何把这个架构迁移到更大场景。  

**C4. 如果让你重构这个项目，你会从哪里开始？**  
- 要点：后端拆出判题服务和 LLM 服务、前端组件化与工程化、增加测试与监控、完善配置管理等。  

**C5. 如果大模型服务完全不可用，你的系统还能提供多少功能？体验会变成怎样？**  
- 要点：判题功能完整保留，现有题库仍可使用；AI 出题与讲解降级，前端给出明显提示。这体现了“核心功能与增强功能解耦”的设计。  

---

## 使用建议

- **复盘阅读顺序**：先看第一部分架构，再看第二部分三个难点，最后用第四部分的问题自测。  
- **面试前一天**：重点背熟第三部分那几条简历 bullet 和你最熟悉的 5–8 道面试问答。  
- **答辩/汇报**：可以从“为什么让大模型做助教而不是裁判”这个角度切入，区别于大部分只会调 API 的项目。  

如果你能把本指南里的内容都用“自己的话”说出来，那么这个项目就不仅是一个课设，而是你可以在中高级面试中拿得出手的“标志性作品”。
